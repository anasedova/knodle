{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TAC-based Relation Extraction dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to preprocess data in CONLL format to use it in Knodle framework.\n",
    "\n",
    "The dataset preproessed here is weakly-supervised dataset built over Knowledge Base Population challenges in the Text Analysis Conference. For development and test purposes the corpus annotated via crowdsourcing and human labeling from KBP is used (Zhang et al. (2017)). The training is done on a weakly-supervised noisy dataset based on TAC KBP corpora (Surdeanu (2013)), also used in Roth (2014). The TAC dataset was annotated with entity pairs extracted from Freebase (Google (2014)) where corresponding relations have been mapped to the 41 TAC relations types as used in the TAC KBP challenges (e.g., per:schools_attended and org:members). The amount of entity pairs per relation was limited to 10.000 and each entity pair is allowed to be mentioned in no more than 500 sentences.\n",
    "\n",
    "Additionally, if no rule matched a sentence, it was added to the dataset with no_relation label. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, let's make some basic imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from typing import Dict\n",
    "from minio import Minio\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "from joblib import dump\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from knodle.trainer.utils import log_section\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "PRINT_EVERY = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the files names\n",
    "Z_MATRIX_OUTPUT_TRAIN = \"train_rule_matches_z.lib\"\n",
    "Z_MATRIX_OUTPUT_DEV = \"dev_rule_matches_z.lib\"\n",
    "Z_MATRIX_OUTPUT_TEST = \"test_rule_matches_z.lib\"\n",
    "\n",
    "T_MATRIX_OUTPUT_TRAIN = \"mapping_rules_labels.lib\"\n",
    "\n",
    "TRAIN_SAMPLES_OUTPUT = \"df_train.lib\"\n",
    "DEV_SAMPLES_OUTPUT = \"df_dev.lib\"\n",
    "TEST_SAMPLES_OUTPUT = \"df_test.lib\"\n",
    "\n",
    "data_path = \"../../data_from_minio\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset, as all datasets provided in knodle, are laoded to minio and can be easily downlaoded with Minio client. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48b79e92b9324e5895a70ddbf6daeca8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-10 17:27:19,113 urllib3.connectionpool DEBUG    Starting new HTTP connection (1): knodle.dm.univie.ac.at:80\n",
      "2021-03-10 17:27:19,246 urllib3.connectionpool DEBUG    http://knodle.dm.univie.ac.at:80 \"HEAD /knodle/datasets/conll/train.conll HTTP/1.1\" 200 0\n",
      "2021-03-10 17:27:19,317 urllib3.connectionpool DEBUG    http://knodle.dm.univie.ac.at:80 \"GET /knodle/datasets/conll/train.conll HTTP/1.1\" 200 2209656369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-127-fb7dc1b1f268>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"minio_files\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     client.fget_object(\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mbucket_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"minio_bucket\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mobject_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"minio_prefix\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/minio/api.py\u001b[0m in \u001b[0;36mfget_object\u001b[0;34m(self, bucket_name, object_name, file_path, request_headers, ssec, version_id, extra_query_params, tmp_file_path)\u001b[0m\n\u001b[1;32m   1096\u001b[0m             )\n\u001b[1;32m   1097\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ab\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtmp_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m                     \u001b[0mtmp_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/urllib3/response.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    574\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_fp_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/urllib3/response.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m                 \u001b[0mcache_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfp_closed\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m                 if (\n\u001b[1;32m    521\u001b[0m                     \u001b[0mamt\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    452\u001b[0m             \u001b[0;31m# Amount is given, implement using readinto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m             \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def get_conll_config():\n",
    "    config = {\n",
    "        \"minio_url\": \"knodle.dm.univie.ac.at\",\n",
    "        \"minio_user\": \"UnM_LN*jSYK74Iz4\",\n",
    "        \"minio_pw\": \"cQOs4|9Dr2_+HuFKneC8@dRgAtrV21i4Dumy\",\n",
    "        \"minio_bucket\": \"knodle\",\n",
    "        \"minio_prefix\": \"datasets/conll\",\n",
    "        \"minio_files\": [\n",
    "            \"train.conll\",\n",
    "            \"dev.conll\",\n",
    "            \"test.conll\",\n",
    "            \"rules.csv\",\n",
    "            \"labels.txt\"\n",
    "        ],\n",
    "        \"data_dir\": data_path,\n",
    "        \"num_features\": 400,\n",
    "        \"num_classes\": 2,\n",
    "    }\n",
    "    return config\n",
    "\n",
    "config = get_conll_config()\n",
    "client = Minio(config.get(\"minio_url\"), secure=False)\n",
    "\n",
    "for file in tqdm(config.get(\"minio_files\")):\n",
    "    client.fget_object(\n",
    "        bucket_name=config.get(\"minio_bucket\"),\n",
    "        object_name=os.path.join(config.get(\"minio_prefix\"), file),\n",
    "        file_path=os.path.join(data_path, \"conll_data\", file),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set paths to input data\n",
    "path_labels = os.path.join(data_path, \"conll_data\", \"labels.txt\")\n",
    "path_rules = os.path.join(data_path, \"conll_data\", \"rules.csv\")\n",
    "path_train_data = os.path.join(data_path, \"conll_data\", \"train.conll\")\n",
    "path_dev_data = os.path.join(data_path, \"conll_data\", \"dev.conll\")\n",
    "path_test_data = os.path.join(data_path, \"conll_data\", \"test.conll\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview dataset¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_file_lines(file_name: str) -> int:\n",
    "    \"\"\" Count the number of line in a file \"\"\"\n",
    "    with open(file_name) as f:\n",
    "        return len(f.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#\tindex\ttoken\tsubj\tsubj_type\tobj\tobj_type\tstanford_pos\tstanford_ner\tstanford_deprel\tstanford_head\n",
      "\n",
      "# id=E0065795:0-pos docid=E0065795:0 reln=org:alternate_names\n",
      "\n",
      "1\tProfile\t_\t_\t_\t_\tVB\tO\tadvmod\t16\n",
      "\n",
      "2\t,\t_\t_\t_\t_\t,\tO\tpunct\t16\n",
      "\n",
      "3\tbasic\t_\t_\t_\t_\tJJ\tO\tamod\t4\n",
      "\n",
      "4\tinformation\t_\t_\t_\t_\tNN\tO\tcompound\t5\n",
      "\n",
      "5\tATG\t_\t_\tOBJECT\tORG\tNNP\tORG\tnsubj\t16\n",
      "\n",
      "6\t(\t_\t_\t_\t_\t-LRB-\tO\tpunct\t11\n",
      "\n",
      "7\tArt\tSUBJECT\tORGANIZATION\t_\t_\tNNP\tORG\tcompound\t9\n",
      "\n",
      "8\tTechnology\tSUBJECT\tORGANIZATION\t_\t_\tNNP\tORG\tcompound\t9\n",
      "\n",
      "9\tGroup\tSUBJECT\tORGANIZATION\t_\t_\tNNP\tORG\tnmod\t11\n",
      "\n",
      "10\t,\t_\t_\t_\t_\t,\tO\tpunct\t11\n",
      "\n",
      "11\tInc.\t_\t_\t_\t_\tNNP\tGPE\tappos\t5\n",
      "\n",
      "12\t,\t_\t_\t_\t_\t,\tO\tpunct\t11\n",
      "\n",
      "13\tNASDAQ\t_\t_\t_\t_\tNNP\tORG\tnpadvmod\t11\n",
      "\n",
      "14\t:\t_\t_\t_\t_\t:\tO\tpunct\t13\n",
      "\n",
      "15\t)\t_\t_\t_\t_\t-RRB-\tO\tpunct\t11\n",
      "\n",
      "16\tmakes\t_\t_\t_\t_\tVBZ\tO\tROOT\t16\n",
      "\n",
      "17\tsoftware\t_\t_\t_\t_\tNN\tO\tdobj\t16\n",
      "\n",
      "18\tand\t_\t_\t_\t_\tCC\tO\tcc\t16\n",
      "\n",
      "19\tdelivers\t_\t_\t_\t_\tVBZ\tO\tconj\t16\n",
      "\n",
      "20\te\t_\t_\t_\t_\tNN\tO\tnmod\t22\n",
      "\n",
      "21\t-\t_\t_\t_\t_\tHYPH\tO\tpunct\t22\n",
      "\n",
      "22\tcommerce\t_\t_\t_\t_\tNN\tO\tnmod\t26\n",
      "\n",
      "23\tand\t_\t_\t_\t_\tCC\tO\tcc\t22\n",
      "\n",
      "24\tWeb\t_\t_\t_\t_\tNN\tO\tcompound\t25\n",
      "\n",
      "25\tmarketing\t_\t_\t_\t_\tNN\tO\tconj\t22\n",
      "\n",
      "26\tsolutions\t_\t_\t_\t_\tNNS\tO\tdobj\t19\n",
      "\n",
      "27\tthat\t_\t_\t_\t_\tIN\tO\tdobj\t31\n",
      "\n",
      "28\tmany\t_\t_\t_\t_\tJJ\tO\tamod\t30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data = open(path_train_data)\n",
    "for i in range(30):\n",
    "    line = train_data.readline()\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's read labels from the file and encode them with ids. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels2ids = {}\n",
    "with open(path_labels, encoding=\"UTF-8\") as file:\n",
    "    for line in file.readlines():\n",
    "        relation, relation_enc = line.replace(\"\\n\", \"\").split(\",\")\n",
    "        labels2ids[relation] = int(relation_enc)\n",
    "\n",
    "num_labels = len(labels2ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to have samples with no rule matched as negative samples, let's heuristically calculate the other class id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_class_id = max(labels2ids.values()) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, we should get the rules. In our case, they are entity pairs extracted from Freebase and stored in the separate csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rule</th>\n",
       "      <th>rule_id</th>\n",
       "      <th>label</th>\n",
       "      <th>label_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Art_Technology_Group ATG</td>\n",
       "      <td>0</td>\n",
       "      <td>org:alternate_names</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Union_Cycliste_Internationale UCI</td>\n",
       "      <td>1</td>\n",
       "      <td>org:alternate_names</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hanwha 한화</td>\n",
       "      <td>2</td>\n",
       "      <td>org:alternate_names</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Radio_Free_Europe Radio_Liberty</td>\n",
       "      <td>3</td>\n",
       "      <td>org:alternate_names</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hermès Hermes</td>\n",
       "      <td>4</td>\n",
       "      <td>org:alternate_names</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                rule  rule_id                label  label_id\n",
       "0           Art_Technology_Group ATG        0  org:alternate_names        25\n",
       "1  Union_Cycliste_Internationale UCI        1  org:alternate_names        25\n",
       "2                          Hanwha 한화        2  org:alternate_names        25\n",
       "3    Radio_Free_Europe Radio_Liberty        3  org:alternate_names        25\n",
       "4                      Hermès Hermes        4  org:alternate_names        25"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rules = pd.read_csv(path_rules)\n",
    "rules.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule2rule_id = dict(zip(rules.rule, rules.rule_id))\n",
    "num_rules = max(rules.rule_id.values) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get rules to classes correspondance matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we need to know which rule corresponds to which class. This information can be got from t_matrix, which is also stored in rules DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "838b4a6e6e2a4ae68c239f46e270be0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['data/mapping_rules_labels.lib']"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_t_matrix(rules: pd.DataFrame, num_labels: int) -> np.ndarray:\n",
    "    \"\"\" Function calculates t matrix (rules x labels) using the known correspondence of relations to decision rules \"\"\"\n",
    "    rule_assignments_t = np.empty([rules.rule_id.max() + 1, num_labels])\n",
    "    for index, row in rules.iterrows():\n",
    "        rule_assignments_t[row[\"rule_id\"], row[\"label_id\"]] = 1\n",
    "    return rule_assignments_t\n",
    "\n",
    "rule_assignments_t = get_t_matrix(rules, num_labels)\n",
    "dump(sp.csr_matrix(rule_assignments_t), os.path.join(data_path, T_MATRIX_OUTPUT_TRAIN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train data should be annotated with rules we already have. Remember, there are no gold labels (as opposite to dev and test data).\n",
    "\n",
    "The annotation is done in the following way: \n",
    "- the sentences are extracted from conll format\n",
    "- the tokens labelled as object and subject are checked whether thery are in rules list\n",
    "- if yes, this sentence is labelled with the corresponding relation\n",
    "- if not, this sentence is labelled with no_relation label (filter_out_other parameter is set to False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ec39b086a7e429096d154b46ca2a2a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-10 17:18:23,628 __main__     INFO     Processed 1.50% of train.conll file\n",
      "2021-03-10 17:18:25,088 __main__     INFO     Processed 3.00% of train.conll file\n",
      "2021-03-10 17:18:26,503 __main__     INFO     Processed 4.51% of train.conll file\n",
      "2021-03-10 17:18:27,940 __main__     INFO     Processed 6.01% of train.conll file\n",
      "2021-03-10 17:18:29,312 __main__     INFO     Processed 7.51% of train.conll file\n",
      "2021-03-10 17:18:30,653 __main__     INFO     Processed 9.01% of train.conll file\n",
      "2021-03-10 17:18:32,011 __main__     INFO     Processed 10.51% of train.conll file\n",
      "2021-03-10 17:18:33,374 __main__     INFO     Processed 12.02% of train.conll file\n",
      "2021-03-10 17:18:34,737 __main__     INFO     Processed 13.52% of train.conll file\n",
      "2021-03-10 17:18:36,103 __main__     INFO     Processed 15.02% of train.conll file\n",
      "2021-03-10 17:18:37,479 __main__     INFO     Processed 16.52% of train.conll file\n",
      "2021-03-10 17:18:38,844 __main__     INFO     Processed 18.02% of train.conll file\n",
      "2021-03-10 17:18:40,221 __main__     INFO     Processed 19.53% of train.conll file\n",
      "2021-03-10 17:18:41,587 __main__     INFO     Processed 21.03% of train.conll file\n",
      "2021-03-10 17:18:42,948 __main__     INFO     Processed 22.53% of train.conll file\n",
      "2021-03-10 17:18:44,280 __main__     INFO     Processed 24.03% of train.conll file\n",
      "2021-03-10 17:18:45,610 __main__     INFO     Processed 25.53% of train.conll file\n",
      "2021-03-10 17:18:46,963 __main__     INFO     Processed 27.03% of train.conll file\n",
      "2021-03-10 17:18:48,317 __main__     INFO     Processed 28.54% of train.conll file\n",
      "2021-03-10 17:18:49,655 __main__     INFO     Processed 30.04% of train.conll file\n",
      "2021-03-10 17:18:51,038 __main__     INFO     Processed 31.54% of train.conll file\n",
      "2021-03-10 17:18:52,462 __main__     INFO     Processed 33.04% of train.conll file\n",
      "2021-03-10 17:18:53,817 __main__     INFO     Processed 34.54% of train.conll file\n",
      "2021-03-10 17:18:55,174 __main__     INFO     Processed 36.05% of train.conll file\n",
      "2021-03-10 17:18:56,544 __main__     INFO     Processed 37.55% of train.conll file\n",
      "2021-03-10 17:18:57,920 __main__     INFO     Processed 39.05% of train.conll file\n",
      "2021-03-10 17:18:59,303 __main__     INFO     Processed 40.55% of train.conll file\n",
      "2021-03-10 17:19:00,699 __main__     INFO     Processed 42.05% of train.conll file\n",
      "2021-03-10 17:19:02,267 __main__     INFO     Processed 43.56% of train.conll file\n",
      "2021-03-10 17:19:03,706 __main__     INFO     Processed 45.06% of train.conll file\n",
      "2021-03-10 17:19:05,075 __main__     INFO     Processed 46.56% of train.conll file\n",
      "2021-03-10 17:19:06,432 __main__     INFO     Processed 48.06% of train.conll file\n",
      "2021-03-10 17:19:07,797 __main__     INFO     Processed 49.56% of train.conll file\n",
      "2021-03-10 17:19:09,199 __main__     INFO     Processed 51.07% of train.conll file\n",
      "2021-03-10 17:19:10,611 __main__     INFO     Processed 52.57% of train.conll file\n",
      "2021-03-10 17:19:11,989 __main__     INFO     Processed 54.07% of train.conll file\n",
      "2021-03-10 17:19:13,347 __main__     INFO     Processed 55.57% of train.conll file\n",
      "2021-03-10 17:19:14,699 __main__     INFO     Processed 57.07% of train.conll file\n",
      "2021-03-10 17:19:16,050 __main__     INFO     Processed 58.58% of train.conll file\n",
      "2021-03-10 17:19:17,425 __main__     INFO     Processed 60.08% of train.conll file\n",
      "2021-03-10 17:19:18,778 __main__     INFO     Processed 61.58% of train.conll file\n",
      "2021-03-10 17:19:20,143 __main__     INFO     Processed 63.08% of train.conll file\n",
      "2021-03-10 17:19:21,524 __main__     INFO     Processed 64.58% of train.conll file\n",
      "2021-03-10 17:19:22,896 __main__     INFO     Processed 66.09% of train.conll file\n",
      "2021-03-10 17:19:24,266 __main__     INFO     Processed 67.59% of train.conll file\n",
      "2021-03-10 17:19:25,613 __main__     INFO     Processed 69.09% of train.conll file\n",
      "2021-03-10 17:19:26,947 __main__     INFO     Processed 70.59% of train.conll file\n",
      "2021-03-10 17:19:28,302 __main__     INFO     Processed 72.09% of train.conll file\n",
      "2021-03-10 17:19:29,658 __main__     INFO     Processed 73.59% of train.conll file\n",
      "2021-03-10 17:19:31,026 __main__     INFO     Processed 75.10% of train.conll file\n",
      "2021-03-10 17:19:32,379 __main__     INFO     Processed 76.60% of train.conll file\n",
      "2021-03-10 17:19:33,740 __main__     INFO     Processed 78.10% of train.conll file\n",
      "2021-03-10 17:19:35,102 __main__     INFO     Processed 79.60% of train.conll file\n",
      "2021-03-10 17:19:36,450 __main__     INFO     Processed 81.10% of train.conll file\n",
      "2021-03-10 17:19:37,776 __main__     INFO     Processed 82.61% of train.conll file\n",
      "2021-03-10 17:19:39,133 __main__     INFO     Processed 84.11% of train.conll file\n",
      "2021-03-10 17:19:40,508 __main__     INFO     Processed 85.61% of train.conll file\n",
      "2021-03-10 17:19:41,917 __main__     INFO     Processed 87.11% of train.conll file\n",
      "2021-03-10 17:19:43,313 __main__     INFO     Processed 88.61% of train.conll file\n",
      "2021-03-10 17:19:44,710 __main__     INFO     Processed 90.12% of train.conll file\n",
      "2021-03-10 17:19:46,099 __main__     INFO     Processed 91.62% of train.conll file\n",
      "2021-03-10 17:19:47,505 __main__     INFO     Processed 93.12% of train.conll file\n",
      "2021-03-10 17:19:48,899 __main__     INFO     Processed 94.62% of train.conll file\n",
      "2021-03-10 17:19:50,266 __main__     INFO     Processed 96.12% of train.conll file\n",
      "2021-03-10 17:19:51,735 __main__     INFO     Processed 97.63% of train.conll file\n",
      "2021-03-10 17:19:53,126 __main__     INFO     Processed 99.13% of train.conll file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def annotate_conll_data_with_lfs(conll_data: str, rule2rule_id: Dict, filter_out_other: bool = True) -> pd.DataFrame:\n",
    "    num_lines = count_file_lines(conll_data)\n",
    "    processed_lines = 0\n",
    "    samples, rules, enc_rules = [], [], []\n",
    "    with open(conll_data, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            processed_lines += 1\n",
    "            line = line.strip()\n",
    "            if line.startswith(\"# id=\"):  # Instance starts\n",
    "                sample = \"\"\n",
    "                subj, obj = {}, {}\n",
    "            elif line == \"\":  # Instance ends\n",
    "                if len(list(subj.keys())) == 0 or len(list(obj.keys())) == 0:\n",
    "                    continue\n",
    "                if min(list(subj.keys())) < min(list(obj.keys())):\n",
    "                    rule = \"_\".join(list(subj.values())) + \" \" + \"_\".join(list(obj.values()))\n",
    "                else:\n",
    "                    rule = \"_\".join(list(subj.values())) + \" \" + \"_\".join(list(obj.values()))\n",
    "                if rule in rule2rule_id.keys():\n",
    "                    samples.append(sample)\n",
    "                    rules.append(rule)\n",
    "                    rule_id = rule2rule_id[rule]\n",
    "                    enc_rules.append(rule_id)\n",
    "                elif not filter_out_other:\n",
    "                    samples.append(sample)\n",
    "                    rules.append(None)\n",
    "                    enc_rules.append(None)\n",
    "                else:\n",
    "                    continue\n",
    "            elif line.startswith(\"#\"):  # comment\n",
    "                continue\n",
    "            else:\n",
    "                splitted_line = line.split(\"\\t\")\n",
    "                token = splitted_line[1]\n",
    "                if splitted_line[2] == \"SUBJECT\":\n",
    "                    subj[splitted_line[0]] = token\n",
    "                    sample += \" \" + token\n",
    "                elif splitted_line[4] == \"OBJECT\":\n",
    "                    obj[splitted_line[0]] = token\n",
    "                    sample += \" \" + token\n",
    "                else:\n",
    "                    sample += \" \" + token\n",
    "            if processed_lines % PRINT_EVERY == 0:\n",
    "                logger.info(\"Processed {:0.2f}% of {} file\".format(100 * processed_lines / num_lines,\n",
    "                                                                   conll_data.split(\"/\")[-1]))\n",
    "                \n",
    "    logger.info(\"Data preprocessing is finished\")\n",
    "    return pd.DataFrame.from_dict({\"samples\": samples, \"rules\": rules, \"enc_rules\": enc_rules})\n",
    "\n",
    "train_data = annotate_conll_data_with_lfs(path_train_data, rule2rule_id, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that we could build a z_matrix for train data and save it as a sparse matrix ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_z_matrix(data: pd.DataFrame, num_rules: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Function calculates the z matrix (samples x rules)\n",
    "    data: pd.DataFrame (samples, matched rules, matched rules id )\n",
    "    output: sparse z matrix\n",
    "    \"\"\"\n",
    "    data_without_nan = data.reset_index().dropna()\n",
    "    z_matrix_sparse = sp.csr_matrix(\n",
    "        (\n",
    "            np.ones(len(data_without_nan['index'].values)),\n",
    "            (data_without_nan['index'].values, data_without_nan['enc_rules'].values)\n",
    "        ),\n",
    "        shape=(len(data.index), num_rules)\n",
    "    )\n",
    "    return z_matrix_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'path_output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-120-5e45a28ebe4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_rule_matches_z\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_z_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_rules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTRAIN_SAMPLES_OUTPUT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_rule_matches_z\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ_MATRIX_OUTPUT_TRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'path_output' is not defined"
     ]
    }
   ],
   "source": [
    "train_rule_matches_z = get_z_matrix(train_data, num_rules)\n",
    "\n",
    "dump(train_data, os.path.join(path_output, TRAIN_SAMPLES_OUTPUT))\n",
    "dump(train_rule_matches_z, os.path.join(path_output, Z_MATRIX_OUTPUT_TRAIN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dev & Test data preprocessing¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dev and test data are to be simply read from the data without any additional annotation since the gold label are known for them.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conll_data_with_labels(\n",
    "        conll_data: str, rule2rule_id: Dict, labels2ids: dict, other_class_id: int = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Processing of TACRED dataset. The function reads the .conll input file, extract the samples and the labels as well\n",
    "    as argument pairs, which are saved as decision rules.\n",
    "    :param conll_data: input data in .conll format\n",
    "    :param rule2rule_id: corresponding of rules to rules ids\n",
    "    :param labels2ids: dictionary of label - id corresponding\n",
    "    :param other_class_id: id of other_class_label\n",
    "    :return: DataFrame with columns \"samples\" (extracted sentences), \"rules\" (entity pairs), \"enc_rules\" (entity pairs\n",
    "            ids), \"labels\" (original labels)\n",
    "    \"\"\"\n",
    "\n",
    "    num_lines = count_file_lines(conll_data)\n",
    "    processed_lines = 0\n",
    "\n",
    "    samples, labels, rules, enc_rules = [], [], [], []\n",
    "    with open(conll_data, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            processed_lines += 1\n",
    "            line = line.strip()\n",
    "            if line.startswith(\"# id=\"):  # Instance starts\n",
    "                sample = \"\"\n",
    "                subj, obj = {}, {}\n",
    "                label = labels2id.get(encode_labels(line.split(\" \")[3][5:], other_class_id)\n",
    "            elif line == \"\":  # Instance ends\n",
    "                if min(list(subj.keys())) < min(list(obj.keys())):\n",
    "                    rule = \"_\".join(list(subj.values())) + \" \" + \"_\".join(list(obj.values()))\n",
    "                else:\n",
    "                    rule = \"_\".join(list(subj.values())) + \" \" + \"_\".join(list(obj.values()))\n",
    "\n",
    "                if rule in rule2rule_id.keys():\n",
    "                    samples.append(sample)\n",
    "                    labels.append(label)\n",
    "                    rules.append(rule)\n",
    "                    rule_id = rule2rule_id[rule]\n",
    "                    enc_rules.append(rule_id)\n",
    "\n",
    "                else:\n",
    "                    samples.append(sample)\n",
    "                    labels.append(label)\n",
    "                    rules.append(None)\n",
    "                    enc_rules.append(None)\n",
    "\n",
    "            elif line.startswith(\"#\"):  # comment\n",
    "                continue\n",
    "            else:\n",
    "                splitted_line = line.split(\"\\t\")\n",
    "                token = splitted_line[1]\n",
    "                if splitted_line[2] == \"SUBJECT\":\n",
    "                    subj[splitted_line[0]] = token\n",
    "                    sample += \" \" + token\n",
    "                elif splitted_line[4] == \"OBJECT\":\n",
    "                    obj[splitted_line[0]] = token\n",
    "                    sample += \" \" + token\n",
    "                else:\n",
    "                    sample += \" \" + token\n",
    "            if processed_lines % PRINT_EVERY == 0:\n",
    "                logger.info(\"Processed {:0.2f}% of {} file\".format(100 * processed_lines / num_lines,\n",
    "                                                                   conll_data.split(\"/\")[-1]))\n",
    "\n",
    "    return pd.DataFrame.from_dict({\"samples\": samples, \"rules\": rules, \"enc_rules\": enc_rules, \"labels\": labels})\n",
    "\n",
    "dev_data = get_conll_data_with_ent_pairs(path_dev_data, rule2rule_id, labels2ids, other_class_id)\n",
    "test_data = get_conll_data_with_ent_pairs(path_test_data, rule2rule_id, labels2ids, other_class_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "z_matrix could be build with the same function as we used for building the z_matrix for train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_rule_matches_z = get_z_matrix(dev_data, num_classes)\n",
    "test_rule_matches_z = get_z_matrix(test_data, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and we save all the data we got. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dev_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-d62fac15ca95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEV_SAMPLES_OUTPUT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_rule_matches_z\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ_MATRIX_OUTPUT_DEV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTEST_SAMPLES_OUTPUT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_rule_matches_z\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ_MATRIX_OUTPUT_TEST\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dev_data' is not defined"
     ]
    }
   ],
   "source": [
    "dump(dev_data, os.path.join(path_output, DEV_SAMPLES_OUTPUT))\n",
    "dump(dev_rule_matches_z, os.path.join(path_output, Z_MATRIX_OUTPUT_DEV))\n",
    "\n",
    "dump(test_data, os.path.join(path_output, TEST_SAMPLES_OUTPUT))\n",
    "dump(test_rule_matches_z, os.path.join(path_output, Z_MATRIX_OUTPUT_TEST))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
