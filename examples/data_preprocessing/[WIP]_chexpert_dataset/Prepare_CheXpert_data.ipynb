{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CheXpert Dataset - Download and Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to download and preprocess the CheXpert dataset for further analysis with weakly supervised experiments. The CheXpert training set is composed of chest radiographs, which were annotated on the basis of reports using the rule-based CheXpert labeler. Each image is labeled with respect 12 pathologies as well as the observations \"No Finding\" and \"Support Devices\". For each of these categories, except \"No Finding\", the assigned weak label is either: (Irvin et al. (2019))\n",
    "\n",
    "- positive (1.0)\n",
    "- negative (0.0)\n",
    "- not mentioned (blank)\n",
    "- uncertain (-1.0) \n",
    "\n",
    "The development set was annotated by radiologists and therefore only contains the binary labels: (Irvin et al. (2019))\n",
    "\n",
    "- positive (1.0) \n",
    "- negative (0.0)\n",
    "\n",
    "You can register for obtaining the data under the following link: https://stanfordmlgroup.github.io/competitions/chexpert/. Once the registration is finished, you should receive an email which contains links for two different versions of the dataset, the original CheXpert dataset (around 439 GB) and a version with downsampled resolution (around 11 GB). The code below uses the downsampled version. Please unzip the downloaded folder in a directory of your choice and don't change the filenames or the folder structure, otherwise you might need to change some of the paths used in the following code in order for it to run properly. The zip file you obtained should contain a training and a validation set. The CheXpert test set is not publicly available, as it is used for the CheXpert competition (see link above). The reports that were used to label the images are also unavailable.\n",
    "\n",
    "Please note that some output of the code below is not displayed due to the data use agreement on https://stanfordmlgroup.github.io/competitions/chexpert/.\n",
    "Please run the code yourself with the data you downloaded from the above link.\n",
    "\n",
    "The original CheXpert paper, \"CheXpert: A large chest radiograph dataset with uncertainty labels and expert comparison\" by Irvin et al. (2019), can be found here: https://arxiv.org/abs/1901.07031."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from tabulate import tabulate\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.utils import save_image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's load the files train.csv and valid.csv, which accompany the images. First, please change the working directory to the appropriate location by inserting the path to the folder in which you stored train.csv and valid.csv where \"data_path\" is mentioned in the code. If you didn't change the folder structure, this path should end with \"\\CheXpert-v1.0-small\\CheXpert-v1.0-small\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data_path\"\n",
    "os.chdir(path) # change working directory to appropriate location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the first five rows of the raw training data. Please note that the small dataframe displayed below is entirely made up of fake entries. If you want to see the true data, please run training_set.head(5) yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Path</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Frontal/Lateral</th>\n",
       "      <th>AP/PA</th>\n",
       "      <th>No Finding</th>\n",
       "      <th>Enlarged Cardiomediastinum</th>\n",
       "      <th>Cardiomegaly</th>\n",
       "      <th>Lung Opacity</th>\n",
       "      <th>Lung Lesion</th>\n",
       "      <th>Edema</th>\n",
       "      <th>Consolidation</th>\n",
       "      <th>Pneumonia</th>\n",
       "      <th>Atelectasis</th>\n",
       "      <th>Pneumothorax</th>\n",
       "      <th>Pleural Effusion</th>\n",
       "      <th>Pleural Other</th>\n",
       "      <th>Fracture</th>\n",
       "      <th>Support Devices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CheXpert-v1.0-small/train/patient99999/study1/...</td>\n",
       "      <td>Female</td>\n",
       "      <td>44</td>\n",
       "      <td>Frontal</td>\n",
       "      <td>AP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CheXpert-v1.0-small/train/patient99999/study2/...</td>\n",
       "      <td>Female</td>\n",
       "      <td>48</td>\n",
       "      <td>Frontal</td>\n",
       "      <td>AP</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Path     Sex  Age  \\\n",
       "0  CheXpert-v1.0-small/train/patient99999/study1/...  Female   44   \n",
       "1  CheXpert-v1.0-small/train/patient99999/study2/...  Female   48   \n",
       "\n",
       "  Frontal/Lateral AP/PA No Finding Enlarged Cardiomediastinum Cardiomegaly  \\\n",
       "0         Frontal    AP        NaN                        NaN          NaN   \n",
       "1         Frontal    AP        1.0                        NaN          NaN   \n",
       "\n",
       "  Lung Opacity Lung Lesion Edema Consolidation Pneumonia Atelectasis  \\\n",
       "0          NaN         NaN  -1.0           NaN       NaN         NaN   \n",
       "1          NaN        -1.0   NaN           NaN       NaN         NaN   \n",
       "\n",
       "  Pneumothorax Pleural Effusion Pleural Other  Fracture Support Devices  \n",
       "0          NaN              NaN           NaN       1.0             NaN  \n",
       "1          NaN              NaN           NaN      -1.0             NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataframe with fake entries for demonstration purposes\n",
    "\n",
    "fake_entries = list(zip([\"CheXpert-v1.0-small/train/patient99999/study1/...\", \"CheXpert-v1.0-small/train/patient99999/study2/...\"],\n",
    "                        [\"Female\", \"Female\"], [44, 48], [\"Frontal\", \"Frontal\"], [\"AP\", \"AP\"], [\"NaN\", 1.0],\n",
    "                        [\"NaN\", \"NaN\"], [\"NaN\", \"NaN\"], [\"NaN\", \"NaN\"], [\"NaN\", -1.0], [-1.0, \"NaN\"], [\"NaN\", \"NaN\"],\n",
    "                        [\"NaN\", \"NaN\"], [\"NaN\", \"NaN\"], [\"NaN\", \"NaN\"], [\"NaN\", \"NaN\"], [\"NaN\", \"NaN\"], \n",
    "                        [1.0, -1.0], [\"NaN\", \"NaN\"]))\n",
    "\n",
    "fake_training_set = pd.DataFrame(data=fake_entries, index=None, columns=training_set.columns, dtype=None, copy=None)\n",
    "\n",
    "fake_training_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations in training set: 223414\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of observations in training set:\", training_set.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, each observation in the training set consists of a path to an image, some additional information about the patient and the nature of the image, as well as the weak labels for all 14 classes. 12 of the classes, \"Enlarged Cardiomediastinum\" to \"Fracture\", are considered pathologies. \"No Finding\" is assigned the label 1 (meaning \"positive\") if no pathology was marked as positive (1.0) or uncertain (-1.0) for this observation. Labels which were blank (meaning that the pathology was not mentioned in the report) turned into NaNs when loading the data. The training set has a total of 223414 observations. (Irvin et al. (2019))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's do the same for the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "validation_set = pd.read_csv(\"valid.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_set.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Path</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Frontal/Lateral</th>\n",
       "      <th>AP/PA</th>\n",
       "      <th>No Finding</th>\n",
       "      <th>Enlarged Cardiomediastinum</th>\n",
       "      <th>Cardiomegaly</th>\n",
       "      <th>Lung Opacity</th>\n",
       "      <th>Lung Lesion</th>\n",
       "      <th>Edema</th>\n",
       "      <th>Consolidation</th>\n",
       "      <th>Pneumonia</th>\n",
       "      <th>Atelectasis</th>\n",
       "      <th>Pneumothorax</th>\n",
       "      <th>Pleural Effusion</th>\n",
       "      <th>Pleural Other</th>\n",
       "      <th>Fracture</th>\n",
       "      <th>Support Devices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CheXpert-v1.0-small/train/patient00000/study1/...</td>\n",
       "      <td>Male</td>\n",
       "      <td>65</td>\n",
       "      <td>Frontal</td>\n",
       "      <td>AP</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CheXpert-v1.0-small/train/patient00001/study2/...</td>\n",
       "      <td>Female</td>\n",
       "      <td>82</td>\n",
       "      <td>Lateral</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Path     Sex  Age  \\\n",
       "0  CheXpert-v1.0-small/train/patient00000/study1/...    Male   65   \n",
       "1  CheXpert-v1.0-small/train/patient00001/study2/...  Female   82   \n",
       "\n",
       "  Frontal/Lateral AP/PA  No Finding  Enlarged Cardiomediastinum  Cardiomegaly  \\\n",
       "0         Frontal    AP         0.0                         0.0           0.0   \n",
       "1         Lateral   NaN         0.0                         0.0           0.0   \n",
       "\n",
       "   Lung Opacity  Lung Lesion  Edema  Consolidation  Pneumonia  Atelectasis  \\\n",
       "0           1.0          0.0    1.0            0.0        0.0          0.0   \n",
       "1           0.0          1.0    0.0            0.0        0.0          0.0   \n",
       "\n",
       "   Pneumothorax  Pleural Effusion  Pleural Other  Fracture  Support Devices  \n",
       "0           0.0               0.0            0.0       0.0              0.0  \n",
       "1           0.0               0.0            0.0       0.0              1.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataframe with fake entries for demonstration purposes\n",
    "\n",
    "fake_entries = list(zip([\"CheXpert-v1.0-small/train/patient00000/study1/...\", \"CheXpert-v1.0-small/train/patient00001/study2/...\"],\n",
    "                        [\"Male\", \"Female\"], [65, 82], [\"Frontal\", \"Lateral\"], [\"AP\", \"NaN\"], [0.0, 0.0],\n",
    "                        [0.0, 0.0], [0.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 0.0], [0.0, 0.0],\n",
    "                        [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 1.0]))\n",
    "\n",
    "fake_validation_set = pd.DataFrame(data=fake_entries, index=None, columns=training_set.columns, dtype=None, copy=None)\n",
    "\n",
    "fake_validation_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations in validation set: 234\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of observations in validation set:\", validation_set.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the validation set has the same structure as the training set. However, the validation set only uses positive (1.0) and negative (0.0) labels and no uncertainty (-1.0) or not-mentioned labels (blank). The validation set contains 234 observations. (Irvin et al. (2019))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, some statistics are computed in order to get an idea about the label distribution in the two datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Number of non-NaN labels    Number of datapoints\n",
      "--------------------------  ----------------------\n",
      "                         3                   64386\n",
      "                         4                   50893\n",
      "                         2                   50672\n",
      "                         5                   23828\n",
      "                         1                   23185\n",
      "                         6                    7922\n",
      "                         7                    1960\n",
      "                         8                     294\n",
      "                         0                     250\n",
      "                         9                      19\n",
      "                        10                       5\n"
     ]
    }
   ],
   "source": [
    "training_labels = training_set.iloc[:, -13:-1]\n",
    "labels_per_row = training_labels.count(axis = 1) # number of non-NaN labels per row in the training set\n",
    "\n",
    "vals = pd.DataFrame(labels_per_row.value_counts())\n",
    "\n",
    "# make a table\n",
    "val_list = [(i, vals[0][i]) for i in vals.index]\n",
    "    \n",
    "print(tabulate(val_list, headers = [\"Number of non-NaN labels\", \"Number of datapoints\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, most training samples have at least a few pathologies, for which they have a label which is not blank, meaning it is either positive (1.0), negative (0.0) or uncertain (-1.0). Nevertheless, there seem to be 250 observations for which none of the 12 pathologies were mentioned in the corresponding report.\n",
    "\n",
    "The following two tables should give an idea about the label distribution for the different pathologies in the training set and in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label distribution in the training set: \n",
      "\n",
      "Pathology                     -1.0    0.0     1.0\n",
      "--------------------------  ------  -----  ------\n",
      "Enlarged Cardiomediastinum   12403  21638   10798\n",
      "Cardiomegaly                  8087  11116   27000\n",
      "Lung Opacity                  5598   6599  105581\n",
      "Lung Lesion                   1488   1270    9186\n",
      "Edema                        12984  20726   52246\n",
      "Consolidation                27742  28097   14783\n",
      "Pneumonia                    18770   2799    6039\n",
      "Atelectasis                  33739   1328   33376\n",
      "Pneumothorax                  3145  56341   19448\n",
      "Pleural Effusion             11628  35396   86187\n",
      "Pleural Other                 2653    316    3523\n",
      "Fracture                       642   2512    9040\n"
     ]
    }
   ],
   "source": [
    "val_list = []\n",
    "for cond in training_labels.columns:\n",
    "    vals = np.array(training_labels.value_counts(subset=[cond]).sort_index(ascending = True))\n",
    "    val_list.append([cond, vals[0], vals[1], vals[2]])\n",
    "\n",
    "print(\"Label distribution in the training set:\", \"\\n\")\n",
    "print(tabulate(val_list, headers = [\"Pathology\", \"-1.0\", \"0.0\", \"1.0\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label distribution in the validation set: \n",
      "\n",
      "Pathology                     0.0    1.0\n",
      "--------------------------  -----  -----\n",
      "Enlarged Cardiomediastinum    125    109\n",
      "Cardiomegaly                  166     68\n",
      "Lung Opacity                  108    126\n",
      "Lung Lesion                   233      1\n",
      "Edema                         189     45\n",
      "Consolidation                 201     33\n",
      "Pneumonia                     226      8\n",
      "Atelectasis                   154     80\n",
      "Pneumothorax                  226      8\n",
      "Pleural Effusion              167     67\n",
      "Pleural Other                 233      1\n",
      "Fracture                      234      0\n"
     ]
    }
   ],
   "source": [
    "validation_labels = validation_set.iloc[:, -13:-1]\n",
    "\n",
    "val_list = []\n",
    "for cond in validation_labels.columns:\n",
    "    vals = np.array(validation_labels.value_counts(subset=[cond]).sort_index(ascending=True))\n",
    "    if cond != \"Fracture\":\n",
    "        val_list.append([cond, vals[0], vals[1]])\n",
    "    else:\n",
    "        val_list.append([cond, vals[0], 0]) # fracture never positively appears in validation set\n",
    "        \n",
    "print(\"Label distribution in the validation set:\", \"\\n\")\n",
    "print(tabulate(val_list, headers = [\"Pathology\", \"0.0\", \"1.0\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, some observations like \"Fracture\" or \"Lung Lesion\", which are positively mentioned (i.e. label 1.0) for numerous observations in the training set, barely appear or don't appear at all in the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have familiarized ourselves with the labels in the training and in the validation set, let's take a look at an example image from the training set. In order to do that, we first need to create paths that lead to the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths to training images\n",
    "image_paths_train = [os.path.join(path[: path.find(\"CheXpert-v1.0-small\")], \"CheXpert-v1.0-small\", p) for p in training_set[\"Path\"]]\n",
    "\n",
    "# paths to validation images\n",
    "image_paths_valid = [os.path.join(path[: path.find(\"CheXpert-v1.0-small\")], \"CheXpert-v1.0-small\", p) for p in validation_set[\"Path\"]]\n",
    "\n",
    "sample_image = Image.open(image_paths_train[0]).convert('RGB')\n",
    "plt.imshow(sample_image)\n",
    "plt.show()\n",
    "print(\"Dimensions of image:\", sample_image.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All images in the (downsampled) dataset have a dimensionality of around 390 x 320 pixels, however, the individual image sizes vary. (Garbin et al. (2021))\n",
    "\n",
    "In the following, a transform sequence is defined, which will be used to preprocess the images. The images are first resized to 224 x 224 pixels, since this is a requirement for many pre-trained CNNs in PyTorch, and then normalized with the mean and standard deviation from ImageNet. Such transformations have previously been applied by several other submissions regarding CheXpert on GitHub such as:\n",
    "[this](https://github.com/gaetandi/cheXpert/blob/master/cheXpert_final.ipynb) and [this](https://github.com/Stomper10/CheXpert/blob/master/CheXpert_DenseNet121_FL.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_list = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # normalization from ImageNet\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following class will be used to transform the images and prepare the dataset for PyTorch's DataLoader. Moreover, the uncertainty labels will be handled according to either the \"U-Ones\", \"U-Zeroes\", \"U-Ignore\" or \"U-MultiClass\" approach as they were described in the origianl CheXpert paper by Irvin et al. (2019).\n",
    "\n",
    "You can specify how the uncertainty labels for each individual pathology will be handled by providing a list of the names of the pathologies to the arguments to_ones, to_zeros and to_ignore. to_ignore changes the uncertainty label (-1.0) to nan. Not specifying any pathology in any of the arguments to_ones, to_zeros or to_ignore will train the model with the \"U-MultiClass\" approach. \n",
    "\n",
    "The \"Support Device\" label as well as the \"No Finding\" label is dropped for all instances, since support devices do not count as pathologies and the \"No Finding\" label only depends on the labels for the remaining 12 pathologies. The \"blank\" labels, indicating that an observation was not mentioned in the corresponding report, will be treated as \"uncertain\".\n",
    "\n",
    "If the return_image parameter is set to False, only the labels are returned. This might be useful if you already have the transformed images saved but wish to experiment with different settings of to_ones, to_zeros or to_ignore.\n",
    "\n",
    "The validation set can be loaded by setting the subset input of the class to \"valid\" and supplying the appropriate image paths and number of images.\n",
    "\n",
    "The structure of the class was inspired by the GitHub submissions credited above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheXpertDatasetProcessor(Dataset):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 path: str,\n",
    "                 subset: str, \n",
    "                 image_paths: List[str], \n",
    "                 number_of_images: int,  \n",
    "                 transform_sequence: list = None,\n",
    "                 to_ones: List[str] = None,\n",
    "                 to_zeros: List[str] = None, \n",
    "                 to_ignore: List[str] = None,\n",
    "                 return_image: bool = True):\n",
    "        \n",
    "        \"\"\"\n",
    "        Args:\n",
    "            path: path to the folder where train.csv and valid.csv are stored\n",
    "            subset: either \"train\" to load the train.csv or \"valid\" to load valid.csv\n",
    "            image_paths: paths to the images\n",
    "            number_of_images: number of images in the dataset\n",
    "            transform_sequence: sequence used to transform the images\n",
    "            to_ones: list of pathologies for which uncertainty labels should be replaced by 1\n",
    "            to_zeros: list of pathologies for which uncertainty labels should be replaced by 0\n",
    "            to_ignore: list of pathologies for which uncertainty labels should be ignored (label will be turned to nan)\n",
    "            return_image: True: image tensor and labels are returned, False: only labels are returned\n",
    "        Returns: \n",
    "            224 x 224 image tensor and a corresponding tensor containing 12 labels\n",
    "        \"\"\"\n",
    "        \n",
    "        self.path = path\n",
    "        self.subset = subset\n",
    "        self.image_paths = image_paths\n",
    "        self.number_of_images = number_of_images\n",
    "        self.transform_sequence = transform_sequence\n",
    "        self.to_ones = to_ones\n",
    "        self.to_zeros = to_zeros\n",
    "        self.to_ignore = to_ignore\n",
    "        self.return_image = return_image\n",
    "        \n",
    "    def process_chexpert_dataset(self):\n",
    "        \n",
    "        # read dataset\n",
    "        if self.subset == \"train\":\n",
    "            data = pd.read_csv(\"train.csv\")\n",
    "            \n",
    "        elif self.subset == \"valid\":\n",
    "            data = pd.read_csv(\"valid.csv\")\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(\"Invalid subset, please choose either 'train' or 'valid'\")\n",
    "            \n",
    "        pathologies = data.iloc[:, -13:-1].columns\n",
    "        \n",
    "        # prepare labels\n",
    "        data.iloc[:, -13:-1] = data.iloc[:, -13:-1].replace(float(\"nan\"), -1) # blank labels -> uncertain\n",
    "        \n",
    "        if self.to_ones:\n",
    "            if all(p in pathologies for p in self.to_ones): # check whether arguments are valid pathologies\n",
    "                data[self.to_ones] = data[self.to_ones].replace(-1, 1) # replace uncertainty labels with ones\n",
    "            else:\n",
    "                raise ValueError(\"List supplied to to_ones contains invalid pathology, please choose from:\",\n",
    "                                 list(pathologies))\n",
    "            \n",
    "        if self.to_zeros:\n",
    "            if all(p in pathologies for p in self.to_zeros):\n",
    "                    data[self.to_zeros] = data[self.to_zeros].replace(-1, 0) # replace uncertainty labels with zeros\n",
    "            else:\n",
    "                raise ValueError(\"List supplied to to_zeros contains invalid pathology, please choose from:\",\n",
    "                                 list(pathologies))\n",
    "            \n",
    "        if self.to_ignore:\n",
    "            if all(p in pathologies for p in self.to_ignore):\n",
    "                    data[self.to_ignore] = data[self.to_ignore].replace(-1, float(\"nan\")) # replace uncertainty labels with nan\n",
    "            else:\n",
    "                raise ValueError(\"List supplied to to_ignore contains invalid pathology, please choose from:\",\n",
    "                                     list(pathologies))\n",
    "        \n",
    "        self.data = data\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        \n",
    "        \"\"\"\n",
    "        index: index of example that should be retrieved\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.return_image not in [True, False]:\n",
    "            raise ValueError(\"Please set return_image argument either to True or False\")\n",
    "        \n",
    "        image_labels = self.data.iloc[index, -13:-1]\n",
    "        \n",
    "        if self.return_image is False: # only labels are returned, not the images\n",
    "            return torch.tensor(image_labels)\n",
    "        \n",
    "        else:\n",
    "            image_name = self.image_paths[index]\n",
    "        \n",
    "            patient_image = Image.open(image_name).convert('RGB')\n",
    "        \n",
    "            if self.transform_sequence:\n",
    "                patient_image = self.transform_sequence(patient_image) # apply the transform_sequence if one is specified\n",
    "        \n",
    "            else:\n",
    "                # even if no other transformation is applied, the image should be turned into a tensor\n",
    "                to_tensor = transforms.ToTensor()\n",
    "                patient_image = to_tensor(patient_image)\n",
    "            \n",
    "            return patient_image, torch.tensor(image_labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.number_of_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training data\n",
    "chexpert_train = CheXpertDatasetProcessor(path = path, subset = \"train\", image_paths = image_paths_train,\n",
    "                                          number_of_images = training_set.shape[0], transform_sequence = transform_list)\n",
    "chexpert_train.process_chexpert_dataset()\n",
    "\n",
    "# prepare validation data\n",
    "chexpert_valid = CheXpertDatasetProcessor(path = path, subset = \"valid\", image_paths = image_paths_valid,\n",
    "                                          number_of_images = validation_set.shape[0], transform_sequence = transform_list)\n",
    "chexpert_valid.process_chexpert_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a given index, the getitem function returns the tensor representing the preprocessed image as well as a tensor containing all of the labels for this observation.\n",
    "\n",
    "Let's take a look at what getitem returns for the training example with index 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chexpert_train.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of image tensor: torch.Size([3, 224, 224])\n",
      "Shape of label tensor: torch.Size([12])\n"
     ]
    }
   ],
   "source": [
    "shape_of_image_tensor = chexpert_train.__getitem__(0)[0].shape\n",
    "shape_of_label_tensor = chexpert_train.__getitem__(0)[1].shape\n",
    "\n",
    "print(\"Shape of image tensor:\", shape_of_image_tensor)\n",
    "print(\"Shape of label tensor:\", shape_of_label_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store the preprocessed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tensors representing the preprocessed images and the tensors containing the labels can be stored on your computer using joblib. If you want to do this, you can run the following code but please be aware that the resulting joblib files might become very large, especially for the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_train = 'chexpert_data_train_labels.joblib'\n",
    "outfile = open(filename_train,'wb')\n",
    "\n",
    "for i in tqdm(range(0, training_set.shape[0])):\n",
    "    joblib.dump(chexpert_train.__getitem__(i)[1], outfile)\n",
    "    \n",
    "filename_valid = 'chexpert_data_valid_labels.joblib'\n",
    "outfile = open(filename_valid,'wb')\n",
    "\n",
    "for i in tqdm(range(0, validation_set.shape[0])):\n",
    "    joblib.dump(chexpert_valid.__getitem__(i)[1], outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes the preprocessing of the CheXpert data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CheXpert: A large chest radiograph dataset with uncertainty labels and expert comparison by Irvin et al. (2019):\n",
    "https://arxiv.org/abs/1901.07031\n",
    "\n",
    "Structured dataset documentation: a datasheet for CheXpert by Garbin et al. (2021):\n",
    "https://arxiv.org/pdf/2105.03020.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
