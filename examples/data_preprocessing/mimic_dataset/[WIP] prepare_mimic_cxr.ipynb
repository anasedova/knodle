{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing of MIMIC-CXR dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting all set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get access to the data, one needs to be a \"credential user\" in PhysioNet and sign the data use agreement. \n",
    "1) To get a credentialed PhysioNet account, follow the following instructions https://physionet.org/about/citi-course/. \n",
    "\n",
    "2) Download the files\n",
    "    \"cxr-record-list.csv.gz\" (from mimic-cxr),\n",
    "    \"cxr-study-list.csv.gz\" (from mimic-cxr),\n",
    "    \"mimic-cxr-2.0.0-split.csv.gz\" (from mimic-cxr-jpg),\n",
    "    \"mimic-cxr-2.0.0-chexpert.csv.gz\" (from mimic-cxr-jpg)\n",
    "    and unzip them with 7zip\n",
    "    \n",
    "3) set your working directory to this folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.chdir(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import random\n",
    "import copy\n",
    "import torch.optim as optim\n",
    "import csv\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import itertools\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading MIMIC-CXR-JPG images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set n between 1 and 377110\n",
    "n = 1000\n",
    "record_list = pd.read_csv(\"cxr-record-list.csv\").to_numpy()\n",
    "study_list = pd.read_csv(\"cxr-study-list.csv\").to_numpy()\n",
    "\n",
    "username = \"your_username_here\"\n",
    "password = \"your_pw_here\"\n",
    "\n",
    "#image download - run only once\n",
    "for i in tqdm(range(n)):\n",
    "    url = [\"wget -r -N -c -np --user=\", username, \" --password=\", password, \" https://physionet.org/files/mimic-cxr-jpg/2.0.0/\",record_list[i,3]]\n",
    "    command = \"\".join(url)\n",
    "    command = \"\".join([command.replace(\".dcm\", \"\"),\".jpg\"])\n",
    "    os.system(command)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading MIMIC-CXR reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = \"your_username_here\"\n",
    "password = \"your_pw_here\"\n",
    "\n",
    "url = [\"wget -r -N -c -np --user=\", username, \" --password=\", password, \" https://physionet.org/files/mimic-cxr/2.0.0/mimic-cxr-reports.zip\"]\n",
    "command = \"\".join(url)\n",
    "os.system(command)\n",
    "\n",
    "'''\n",
    "#Now unzip the folder with 7zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### extracting \"Findings\" and \"Impressions\" from each txt file and saving it to one csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mimic_cxr_text.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    for i in tqdm(range(len(study_list))):\n",
    "        with open(''.join([\"mimic-cxr-reports/\", study_list[i,2]])) as f_path:\n",
    "            text = ''.join(f_path.readlines())\n",
    "        text = text.replace(\"\\n\", \"\")\n",
    "        text = text.replace(\",\", \"\")\n",
    "        start = text.find(\"FINDINGS:\")\n",
    "        end = text.find(\"IMPRESSION:\")\n",
    "        findings = text[start:end]\n",
    "        impressions = text[end:len(text)]\n",
    "        row = [study_list[i,0],study_list[i,1], findings, impressions]\n",
    "        csvwriter = csv.writer(f)\n",
    "        csvwriter.writerow(row)\n",
    "\n",
    "#open\n",
    "reports = pd.read_csv(\"mimic_cxr_text.csv\", names = [\"patient\",\"id\", \"findings\", \"impressions\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding labels and split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mimic-cxr-2.0.0-chexpert.csv contains one of the four values: 1.0,-1.0, 0.0 or missing. They have the following interpretation:\n",
    "\n",
    "* 1.0 The label was positively mentioned in the associated study, and is present in one or more of the corresponding images\n",
    "* 0.0 The label was negatively mentioned in the associated study, and therefore should not be present in any of the corresponding images\n",
    "* -1.0 The label was either: (1) mentioned with uncertainty in the report, and therefore may or may not be present to some degree in the corresponding image, or (2) mentioned with ambiguous language in the report and it is unclear if the pathology exists or not\n",
    "* Missing (empty element) - No mention of the label was made in the report\n",
    "\n",
    "So we are primarily interested in the 1.0s.\n",
    "One study can have multiple labels positively mentioned, like it is the case in row 7 below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "split_list = pd.read_csv(\"mimic-cxr-2.0.0-split.csv\")\n",
    "labels_chexpert = pd.read_csv(\"mimic-cxr-2.0.0-chexpert.csv\")\n",
    "record_list = pd.read_csv(\"cxr-record-list.csv\")\n",
    "\n",
    "labels_chexpert.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there is exactly one label positively mentioned, this label will be assigned to the study. If there are multiple labels positively mentioned, one label will be chosen randomly. When there is non 1.0 assigned to the study, the label will be set to 'No Finding'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialise labels with 0\n",
    "labels_chexpert['label'] = 0\n",
    "labels_list = labels_chexpert.columns.to_numpy()\n",
    "#iterate through labels: \n",
    "#three cases: only one, non, or multiple diagnoses\n",
    "for i in tqdm(range(len(labels_chexpert))):\n",
    "    #which labels are 1? \n",
    "    label_is1 = labels_chexpert.iloc[i,:] == 1.0\n",
    "    if (sum(label_is1)==1):\n",
    "       labels_chexpert.iloc[i,16] = labels_list[label_is1]\n",
    "    elif sum(label_is1) > 1:\n",
    "        labels_chexpert.iloc[i,16] = random.choice(labels_list[label_is1])\n",
    "    else: \n",
    "        labels_chexpert.iloc[i,16] = 'No Finding'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the records, labels and split are merged to one file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge records, labels and split\n",
    "record_split_list = pd.merge(record_list, split_list, how = 'left', on = ['dicom_id', 'study_id','subject_id'])\n",
    "record_split_label_list = pd.merge(record_split_list, labels_chexpert.iloc[:,[0,1,16]], how = 'left', on = ['study_id','subject_id'])\n",
    "\n",
    "print(\"train-val-test split proportions:\" ,record_split_label_list.groupby('split').size()/len(record_split_label_list))\n",
    "print(\"classes proportions:\", record_split_label_list.groupby('label').size()/len(record_split_label_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 14 classes, where about 40% of the studies are assigned to the class 'No finding'. We should keep in mind that we are dealing with an unbalanced dataset.\n",
    "\n",
    "Now the labels get replaced by their id. This gives us the final \"input_list\". Since we will need this file multiple times, it is cached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {'Atelectasis':0,\n",
    "           'Cardiomegaly':1,\n",
    "           'Consolidation':2,\n",
    "           'Edema':3,\n",
    "           'Enlarged Cardiomediastinum':4,\n",
    "           'Fracture':5,\n",
    "           'Lung Lesion':6,\n",
    "           'Lung Opacity':7,\n",
    "           'Pleural Effusion':8,\n",
    "           'Pneumonia':9,\n",
    "           'Pneumothorax':10,\n",
    "           'Pleural Other':11,\n",
    "           'Support Devices':12,\n",
    "           'No Finding':13}\n",
    "\n",
    "for i in tqdm(range(len(record_split_label_list))):\n",
    " record_split_label_list.iloc[i,5] = labels.get(record_split_label_list.iloc[i,5])\n",
    "              \n",
    "input_list = record_split_label_list.to_numpy()\n",
    "#save the whole file\n",
    "np.save(\"input_list.npy\",input_list)\n",
    "#open only first n rows\n",
    "input_list = np.load('input_list.npy', allow_pickle=True)[0:n,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating rules from Chexpert-labler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using the \"phrases\" from Chexpert-labler https://github.com/stanfordmlgroup/chexpert-labeler/tree/master/phrases for building our rules. Therefore, we need to download the .txt files corresponding to each class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download synonym list from chexpert\n",
    "classes = list(labels)\n",
    "#lower case\n",
    "classes = [each_string.lower() for each_string in classes]\n",
    "#replace whitespace with _\n",
    "classes = [each_string.replace(\" \", \"_\") for each_string in classes]\n",
    "labels2ids = {classes[i]:i for i in range(14)}\n",
    "#create folder\n",
    "os.makedirs(\"\".join([os.getcwd(),\"/chexpert_rules\"]))\n",
    "#store files in folder\n",
    "for i in range(len(classes)):\n",
    "    os.system(\"\".join([\"curl https://raw.githubusercontent.com/stanfordmlgroup/chexpert-labeler/master/phrases/mention/\", \n",
    "                       classes[i], \".txt \", \"-o chexpert_rules/\", classes[i], \".txt\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The T matrix contains information about which rule corresponds to which label. In the following snippets, we build this matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read txt in\n",
    "lines = {}\n",
    "for i in range(len(classes)):\n",
    "    with open(\"\".join([\"chexpert_rules/\", classes[i], \".txt\"])) as f:\n",
    "        lines[classes[i]] = [each_string.replace(\"\\n\", \"\") for each_string in f.readlines()]\n",
    "          \n",
    "mentions = pd.DataFrame({'label': label, 'rule': rule} for (label, rule) in lines.items())\n",
    "mentions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building the dataframe \"rules\"\n",
    "rules = pd.DataFrame([i for i in itertools.chain.from_iterable(mentions['rule'])], columns = [\"rule\"])\n",
    "rules['rule_id'] = range(len(rules))\n",
    "rules['label'] = np.concatenate([np.repeat(mentions['label'][i], len(mentions['rule'][i])) for i in range(14)])\n",
    "rules['label_id'] = [labels2ids[rules['label'][i]] for i in range(len(rules))]\n",
    "rules.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule2rule_id = dict(zip(rules[\"rule\"], rules[\"rule_id\"]))\n",
    "rule2label = dict(zip(rules[\"rule_id\"], rules[\"label_id\"]))\n",
    "\n",
    "def get_mapping_rules_labels_t(rule2label: Dict, num_classes: int) -> np.ndarray:\n",
    "    \"\"\" Function calculates t matrix (rules x labels) using the known correspondence of relations to decision rules \"\"\"\n",
    "    mapping_rules_labels_t = np.zeros([len(rule2label), num_classes])\n",
    "    for rule, labels in rule2label.items():\n",
    "        mapping_rules_labels_t[rule, labels] = 1\n",
    "    return mapping_rules_labels_t\n",
    "\n",
    "mapping_rules_labels_t = get_mapping_rules_labels_t(rule2label, len(labels2ids))\n",
    "mapping_rules_labels_t[0:5,:]\n",
    "mapping_rules_labels_t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to check if there are any rules which assign to the same class. Indeed, there is one case, rule \"defib\" builds a rule for two different classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(rules['rule'])) == len(rules['rule'])\n",
    "rules_size = rules.groupby('rule').size() \n",
    "rules_size[np.where(rules_size > 1)[0]]\n",
    "#rule defib appears for two different classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image encoding \n",
    "## Finetuning a pretrained CNN and extracting the second last layer as features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the image encoding, we use the concept of transfer learning. \n",
    "Therefore, we take a pretrained CNN and continue training the model with our data. This saves a lot of time and leads to satisfying results even on a small dataset. \n",
    "For the implementation we use pytorch and the pretrained resnet50 from torchvision. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following class loads the data and transforms it in the way it is required for resnet50. It is written in the form such that it is compatible with torch.utils.data.DataLoader. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mimicDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, path):\n",
    "        'Initialization'\n",
    "        self.path = path\n",
    "        #self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.path)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        image = Image.open(\"\".join([\"physionet.org/files/mimic-cxr-jpg/2.0.0/\",self.path[index,3].replace(\".dcm\", \".jpg\")])).convert('RGB')\n",
    "        X = self.transform(image)\n",
    "        label = self.path[index,5]\n",
    "        \n",
    "        return X, torch.tensor(label)\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we split the data according to the split given in mimic-cxr-jpg. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_train = input_list[input_list[:,4] == 'train',:]\n",
    "input_validate = input_list[input_list[:,4] == 'validate',:]\n",
    "input_test = input_list[input_list[:,4] == 'test',:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the dataset is unbalanced, we use a weighted sampler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = np.zeros(14)\n",
    "for i in range(14): class_counts[i] = sum(input_train[:,5]==i)\n",
    "weight = 1/class_counts\n",
    "sample_weights = np.array([weight[t] for t in input_train[:,5]])\n",
    "sample_weights = torch.from_numpy(sample_weights)\n",
    "sample_weights = sample_weights.double()\n",
    "sampler = torch.utils.data.WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {'train' : mimicDataset(input_train),\n",
    "           'val': mimicDataset(input_validate),\n",
    "           'test':  mimicDataset(input_test)}\n",
    "\n",
    "dataloaders = {'train': DataLoader(dataset['train'] , batch_size=4, num_workers=0, sampler = sampler),\n",
    "               'val': DataLoader(dataset['val'] , batch_size=4, num_workers=0 )}\n",
    "\n",
    "dataset_sizes = {x: len(dataset[x]) for x in ['train', 'val']}\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is taken from https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                if phase == 'val':\n",
    "                    print('predictions',preds)\n",
    "\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    print('Best val Acc: {:4f}'.format(best_acc), )\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet50(pretrained=True)\n",
    "num_ftrs = model.fc.in_features\n",
    "# set output size to 14 (number of classes)\n",
    "model.fc = nn.Linear(num_ftrs, 14)\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "step_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "model = train_model(model, criterion, optimizer, step_lr_scheduler, num_epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that we have trained our model, we will extract the features. \n",
    "In the next step, the last layer is removed from the model, so the output is then the second last layer with dimension 1x2048.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modules = list(model.children())[:-1]\n",
    "model=torch.nn.Sequential(*modules)\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "    \n",
    "model.eval()\n",
    "#apply modified resnet50 to data\n",
    "dataloaders = DataLoader(mimicDataset(input_list[:n,:]), batch_size=n,num_workers=0)\n",
    "    \n",
    "data, labels = next(iter(dataloaders))\n",
    "with torch.no_grad():\n",
    "    features_var = model(data)\n",
    "    features = features_var.data \n",
    "    all_X = features.reshape(n,2048).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump\n",
    "#only features\n",
    "dump(all_X, \"all_X.lib\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
