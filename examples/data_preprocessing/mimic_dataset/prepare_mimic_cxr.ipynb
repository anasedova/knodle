{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing of MIMIC-CXR dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook illustrates how week supervision can be applied on X-rays and radiology reports.  \n",
    "[MIMIC-CXR](https://physionet.org/content/mimic-cxr/2.0.0/) Database is a large publicly available dataset of chest X-rays including radiology reports. It contains 377110 images and 227835 radiographic studies.\n",
    "[MIMIC-CXR-JPG](https://physionet.org/content/mimic-cxr-jpg/2.0.0/) Database also includes weak labels which are derived from the radiology reports using CheXpert labler. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting all set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get access to the data, one needs to be a \"credential user\" in PhysioNet and sign the data use agreement. \n",
    "1. To get a credentialed PhysioNet account, follow the following [instructions](https://physionet.org/about/citi-course/). \n",
    "2. Then sign the data use agreement for both, the MIMIC-CXR and the MIMIC-CXR-JPG database. \n",
    "3. Download the files\n",
    "    \"cxr-record-list.csv.gz\" (from mimic-cxr),\n",
    "    \"cxr-study-list.csv.gz\" (from mimic-cxr),\n",
    "    \"mimic-cxr-2.0.0-split.csv.gz\" (from mimic-cxr-jpg),\n",
    "    \"mimic-cxr-2.0.0-chexpert.csv.gz\" (from mimic-cxr-jpg)\n",
    "    and unzip them with 7zip   \n",
    "4. set your working directory to this folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.chdir(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import copy\n",
    "import torch.optim as optim\n",
    "import csv\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from typing import Dict\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading MIMIC-CXR-JPG images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we are going to download the first part of our dataset: X-rays images. For that, you will need credentialed PhysioNet account. Before executing the following please:\n",
    "1. check if you have enough memory space in the current environment (the first 1000 images are 1.51GB large)\n",
    "2. check if wget is installed on your device\n",
    "3. set n\n",
    "4. insert your physionet username and password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set n between 1 and 377110\n",
    "n = 1000\n",
    "record_list = pd.read_csv(\"cxr-record-list.csv\").to_numpy()\n",
    "study_list = pd.read_csv(\"cxr-study-list.csv\").to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = \"your_username_here\"\n",
    "password = \"your_pw_here\"\n",
    "\n",
    "#image download - run only once\n",
    "for i in tqdm(range(n)):\n",
    "    url = [\"wget -r -N -c -np --user=\", username, \" --password=\", password, \" https://physionet.org/files/mimic-cxr-jpg/2.0.0/\",record_list[i,3]]\n",
    "    command = \"\".join(url)\n",
    "    command = \"\".join([command.replace(\".dcm\", \"\"),\".jpg\"])\n",
    "    os.system(command)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading MIMIC-CXR reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we download the reports. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#report download - run only once\n",
    "url = [\"wget -r -N -c -np --user=\", username, \" --password=\", password, \" https://physionet.org/files/mimic-cxr/2.0.0/mimic-cxr-reports.zip\"]\n",
    "command = \"\".join(url)\n",
    "os.system(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unzip the mimic-cxr-reports folder using 7zip."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the reports are all stored in separate files, we process them such that we get one csv where each report corresponds to one row. We extract the \"Findings\" and \"Impressions\" sections from each txt file and save it to one csv. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mimic_cxr_text.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    for i in tqdm(range(len(study_list))):\n",
    "        with open(''.join([\"mimic-cxr-reports/\", study_list[i,2]])) as f_path:\n",
    "            text = ''.join(f_path.readlines())\n",
    "        text = text.replace(\"\\n\", \"\")\n",
    "        text = text.replace(\",\", \"\")\n",
    "        start = text.find(\"FINDINGS:\")\n",
    "        end = text.find(\"IMPRESSION:\")\n",
    "        findings = text[start:end]\n",
    "        impressions = text[end:len(text)]\n",
    "        row = [study_list[i,0],study_list[i,1], findings, impressions]\n",
    "        csvwriter = csv.writer(f)\n",
    "        csvwriter.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open\n",
    "reports = pd.read_csv(\"mimic_cxr_text.csv\", names = [\"subject_id\",\"study_id\", \"findings\", \"impressions\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the section \"Findings\" contains a description of the important aspects in the image and the section \"Impression\" gives a short summary of the most immediately relevant findings. For our rules, we use the \"Impression\" section, or if impressions are not available the \"Finding\" sections. About 5% of the reports are formated differently(do not have impression or findings sections), these we ignore in this analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of NAs in findings and impressions: findings       78174\n",
      "impressions    39692\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-524708893a8e>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reports['impressions'][loc] = np.nan\n",
      "C:\\Users\\marli\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "#missing values are denoted by \".\" in impressions -> nan\n",
    "loc = np.where(reports['impressions'] == '.')[0]\n",
    "reports['impressions'][loc] = np.nan\n",
    "print(\"number of NAs in findings and impressions:\", pd.isna(reports[['findings', 'impressions']]).sum())\n",
    "\n",
    "#if impression is missing insert finding\n",
    "reports.impressions.fillna(reports.findings, inplace=True)\n",
    "#if non of both are there, we do not analyse this study\n",
    "del reports['findings']\n",
    "reports_processed = reports.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some reports refer to multiple images. Now we merge the reports to our record list such that each image gets a report assigned if a report is available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge reports to record_list\n",
    "record_list = pd.read_csv(\"cxr-record-list.csv\")\n",
    "record_report_list = pd.merge(record_list, reports_processed, how = 'left', on= ['study_id','subject_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding labels and split\n",
    "Now we want to build an array, which contains all needed information of the images. The \"cxr_record_list.csv\" contains the columns (`subject_id`,`study_id`,`dicom_id`) which all together form a unique ID and `path`. Now we want to merge the labels and the split information to each image.  \n",
    "\n",
    "But before we can do that, we need to specify the labels:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CheXpert labler detect the presence of 14 diagnoses in radiology reports. \n",
    "\"mimic-cxr-2.0.0-chexpert.csv\" contains one column for each of the 14 diagnoses and one row for each study. Each study and each diagnosis has assigned one of the four values: 1.0,-1.0, 0.0 or missing. They have the following interpretation:\n",
    "\n",
    "* 1.0 The label was positively mentioned in the associated study, and is present in one or more of the corresponding images\n",
    "* 0.0 The label was negatively mentioned in the associated study, and therefore should not be present in any of the corresponding images\n",
    "* -1.0 The label was either: (1) mentioned with uncertainty in the report, and therefore may or may not be present to some degree in the corresponding image, or (2) mentioned with ambiguous language in the report and it is unclear if the pathology exists or not\n",
    "* Missing (empty element) - No mention of the label was made in the report\n",
    "\n",
    "So we are primarily interested in the 1.0s.\n",
    "One study can have multiple labels positively mentioned, like it is the case in row 7 below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Atelectasis</th>\n",
       "      <th>Cardiomegaly</th>\n",
       "      <th>Consolidation</th>\n",
       "      <th>Edema</th>\n",
       "      <th>Enlarged Cardiomediastinum</th>\n",
       "      <th>Fracture</th>\n",
       "      <th>Lung Lesion</th>\n",
       "      <th>Lung Opacity</th>\n",
       "      <th>No Finding</th>\n",
       "      <th>Pleural Effusion</th>\n",
       "      <th>Pleural Other</th>\n",
       "      <th>Pneumonia</th>\n",
       "      <th>Pneumothorax</th>\n",
       "      <th>Support Devices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Atelectasis  Cardiomegaly  Consolidation  Edema  \\\n",
       "0          NaN           NaN            NaN    NaN   \n",
       "1          NaN           NaN            NaN    NaN   \n",
       "2          NaN           NaN            NaN    NaN   \n",
       "3          NaN           NaN            NaN    NaN   \n",
       "4          NaN           NaN            1.0    NaN   \n",
       "5          NaN           NaN            NaN    NaN   \n",
       "6          NaN           NaN            NaN    NaN   \n",
       "7          NaN           NaN            NaN   -1.0   \n",
       "\n",
       "   Enlarged Cardiomediastinum  Fracture  Lung Lesion  Lung Opacity  \\\n",
       "0                         NaN       NaN          NaN           NaN   \n",
       "1                         NaN       NaN          NaN           NaN   \n",
       "2                         NaN       NaN          NaN           NaN   \n",
       "3                         NaN       NaN          NaN           NaN   \n",
       "4                         NaN       NaN          NaN           NaN   \n",
       "5                         NaN       NaN          NaN           NaN   \n",
       "6                         NaN       NaN          NaN           NaN   \n",
       "7                         NaN       NaN          NaN          -1.0   \n",
       "\n",
       "   No Finding  Pleural Effusion  Pleural Other  Pneumonia  Pneumothorax  \\\n",
       "0         1.0               NaN            NaN        NaN           NaN   \n",
       "1         1.0               NaN            NaN        NaN           NaN   \n",
       "2         1.0               NaN            NaN        NaN           NaN   \n",
       "3         1.0               NaN            NaN        NaN           NaN   \n",
       "4         NaN               NaN            NaN       -1.0           NaN   \n",
       "5         1.0               NaN            NaN        NaN           NaN   \n",
       "6         1.0               NaN            NaN        NaN           NaN   \n",
       "7         NaN               1.0            NaN        1.0           NaN   \n",
       "\n",
       "   Support Devices  \n",
       "0              NaN  \n",
       "1              NaN  \n",
       "2              NaN  \n",
       "3              NaN  \n",
       "4              NaN  \n",
       "5              NaN  \n",
       "6              NaN  \n",
       "7              NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_list = pd.read_csv(\"mimic-cxr-2.0.0-split.csv\")\n",
    "labels_chexpert = pd.read_csv(\"mimic-cxr-2.0.0-chexpert.csv\")\n",
    "\n",
    "labels_chexpert.iloc[:,2:].head(8) #note that we removed the ids in the output due to data privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get one label for each study, we do the following:\n",
    "If there is exactly one label positively mentioned, this label will be assigned to the study. If there are multiple labels positively mentioned, one label will be chosen randomly. When there is no 1.0 assigned to the study, the label will be set to 'No Finding'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 227827/227827 [10:35<00:00, 358.71it/s]\n"
     ]
    }
   ],
   "source": [
    "#initialise labels with 0\n",
    "labels_chexpert['label'] = 0\n",
    "labels_list = labels_chexpert.columns.to_numpy()\n",
    "#iterate through labels: \n",
    "#three cases: only one, non, or multiple diagnoses\n",
    "for i in tqdm(range(len(labels_chexpert))):\n",
    "    #which labels are 1? \n",
    "    label_is1 = labels_chexpert.iloc[i,:] == 1.0\n",
    "    if (sum(label_is1)==1):\n",
    "       labels_chexpert.iloc[i,16] = labels_list[label_is1]\n",
    "    elif sum(label_is1) > 1:\n",
    "        labels_chexpert.iloc[i,16] = random.choice(labels_list[label_is1])\n",
    "    else: \n",
    "        labels_chexpert.iloc[i,16] = 'No Finding'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          No Finding\n",
       "1          No Finding\n",
       "2          No Finding\n",
       "3          No Finding\n",
       "4       Consolidation\n",
       "5          No Finding\n",
       "6          No Finding\n",
       "7    Pleural Effusion\n",
       "Name: label, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_chexpert['label'].head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 227827/227827 [08:40<00:00, 437.50it/s]\n"
     ]
    }
   ],
   "source": [
    "#change names to ids\n",
    "labels = {'Atelectasis':0,\n",
    "               'Cardiomegaly':1,\n",
    "               'Consolidation':2,\n",
    "               'Edema':3,\n",
    "               'Enlarged Cardiomediastinum':4,\n",
    "               'Fracture':5,\n",
    "               'Lung Lesion':6,\n",
    "               'Lung Opacity':7,\n",
    "               'Pleural Effusion':8,\n",
    "               'Pneumonia':9,\n",
    "               'Pneumothorax':10,\n",
    "               'Pleural Other':11,\n",
    "               'Support Devices':12,\n",
    "               'No Finding':13}        \n",
    "        \n",
    "for i in tqdm(range(len(labels_chexpert))):\n",
    " labels_chexpert.iloc[i,16] = labels.get(labels_chexpert.iloc[i,16])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge labels and splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the records, reports, labels and split are merged to one file. Note that sometimes one report refers to multiple X-rays (e.g. one frontal and one lateral X-ray) and since the labels are derived from the reports, one label might refer to multiple images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge records, labels and split\n",
    "record_report_split_list = pd.merge(record_report_list, split_list, how = 'left', on = ['dicom_id', 'study_id','subject_id'])\n",
    "record_report_split_label_list = pd.merge(record_report_split_list, labels_chexpert.iloc[:,[0,1,16]], how = 'left', on = ['study_id','subject_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train/validation/test split is taken from mimic-cxr-jpg, which is intended to provide a reference dataset split for studies using this data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-val-test split proportions: split\n",
      "test        0.013680\n",
      "train       0.978388\n",
      "validate    0.007931\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"train-val-test split proportions:\" ,record_report_split_label_list.groupby('split').size()/len(record_report_split_label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-val-test split proportions: split\n",
      "test        0.013680\n",
      "train       0.978388\n",
      "validate    0.007931\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"train-val-test split proportions:\" ,record_report_split_label_list.groupby('split').size()/len(record_report_split_label_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 14 classes, where about 42% of the studies are assigned to the class 'No finding'. We should keep in mind that we are dealing with an unbalanced dataset.\n",
    "\n",
    "Now the labels get replaced by their id. This gives us the final \"input_list\". Since we will need this file multiple times and the runtime is relatively long, it is cached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_list_full = record_report_split_label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input_list.lib']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#save the whole file\n",
    "dump(input_list_full, \"input_list.lib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will continue working with the input list of the first n rows where the rows including missing values are dropped. Instead of n=1000 we now have n=932"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "932\n"
     ]
    }
   ],
   "source": [
    "#open only first n rows\n",
    "input_list_pd = load(\"input_list.lib\").iloc[:n,:]\n",
    "#drop nas\n",
    "input_list = input_list_pd.dropna().to_numpy()\n",
    "#save new n\n",
    "n = len(input_list)\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating rules from Chexpert-labler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using the [\"mentions\" from Chexpert-labler](https://github.com/stanfordmlgroup/chexpert-labeler/tree/master/phrases/mention) for building our rules. Therefore, we need to download the .txt files corresponding to each class from github. The file names are the labels we defined above, where all letters are lowercase and instead of whitespaces there are underscores. So first we do some transformations to get a list of our classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download synonym list from chexpert\n",
    "classes = list(labels)\n",
    "#lower case\n",
    "classes = [each_string.lower() for each_string in classes]\n",
    "#replace whitespace with _\n",
    "classes = [each_string.replace(\" \", \"_\") for each_string in classes]\n",
    "labels2ids = {classes[i]:i for i in range(14)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a new folder in our directory and store the txt files there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create folder chexpert_rules\n",
    "os.makedirs(\"\".join([os.getcwd(),\"/chexpert_rules\"]))\n",
    "#store files in folder\n",
    "for i in range(len(classes)):\n",
    "    os.system(\"\".join([\"curl https://raw.githubusercontent.com/stanfordmlgroup/chexpert-labeler/master/phrases/mention/\", \n",
    "                       classes[i], \".txt \", \"-o chexpert_rules/\", classes[i], \".txt\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T Matrix\n",
    "The T matrix contains information about which rule corresponds to which label. In the following snippets, we build this matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>rule</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>atelectasis</td>\n",
       "      <td>[atelecta, collapse]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cardiomegaly</td>\n",
       "      <td>[cardiomegaly, the heart, heart size, cardiac ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>consolidation</td>\n",
       "      <td>[consolidat]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>edema</td>\n",
       "      <td>[edema, heart failure, chf, vascular congestio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>enlarged_cardiomediastinum</td>\n",
       "      <td>[_mediastinum, cardiomediastinum, contour, med...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        label  \\\n",
       "0                 atelectasis   \n",
       "1                cardiomegaly   \n",
       "2               consolidation   \n",
       "3                       edema   \n",
       "4  enlarged_cardiomediastinum   \n",
       "\n",
       "                                                rule  \n",
       "0                               [atelecta, collapse]  \n",
       "1  [cardiomegaly, the heart, heart size, cardiac ...  \n",
       "2                                       [consolidat]  \n",
       "3  [edema, heart failure, chf, vascular congestio...  \n",
       "4  [_mediastinum, cardiomediastinum, contour, med...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read txt in\n",
    "lines = {}\n",
    "for i in range(len(classes)):\n",
    "    with open(\"\".join([\"chexpert_rules/\", classes[i], \".txt\"])) as f:\n",
    "        lines[classes[i]] = [each_string.replace(\"\\n\", \"\") for each_string in f.readlines()]\n",
    "          \n",
    "mentions = pd.DataFrame({'label': label, 'rule': rule} for (label, rule) in lines.items())\n",
    "mentions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rule</th>\n",
       "      <th>rule_id</th>\n",
       "      <th>label</th>\n",
       "      <th>label_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>atelecta</td>\n",
       "      <td>0</td>\n",
       "      <td>atelectasis</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>collapse</td>\n",
       "      <td>1</td>\n",
       "      <td>atelectasis</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cardiomegaly</td>\n",
       "      <td>2</td>\n",
       "      <td>cardiomegaly</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the heart</td>\n",
       "      <td>3</td>\n",
       "      <td>cardiomegaly</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>heart size</td>\n",
       "      <td>4</td>\n",
       "      <td>cardiomegaly</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           rule  rule_id         label  label_id\n",
       "0      atelecta        0   atelectasis         0\n",
       "1      collapse        1   atelectasis         0\n",
       "2  cardiomegaly        2  cardiomegaly         1\n",
       "3     the heart        3  cardiomegaly         1\n",
       "4    heart size        4  cardiomegaly         1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#building the dataframe \"rules\"\n",
    "rules = pd.DataFrame([i for i in itertools.chain.from_iterable(mentions['rule'])], columns = [\"rule\"])\n",
    "rules['rule_id'] = range(len(rules))\n",
    "rules['label'] = np.concatenate([np.repeat(mentions['label'][i], len(mentions['rule'][i])) for i in range(14)])\n",
    "rules['label_id'] = [labels2ids[rules['label'][i]] for i in range(len(rules))]\n",
    "rules.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T matrix: [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "shape: (148, 14)\n"
     ]
    }
   ],
   "source": [
    "rule2rule_id = dict(zip(rules[\"rule\"], rules[\"rule_id\"]))\n",
    "rule2label = dict(zip(rules[\"rule_id\"], rules[\"label_id\"]))\n",
    "\n",
    "def get_mapping_rules_labels_t(rule2label: Dict, num_classes: int) -> np.ndarray:\n",
    "    \"\"\" Function calculates t matrix (rules x labels) using the known correspondence of relations to decision rules \"\"\"\n",
    "    mapping_rules_labels_t = np.zeros([len(rule2label), num_classes])\n",
    "    for rule, labels in rule2label.items():\n",
    "        mapping_rules_labels_t[rule, labels] = 1\n",
    "    return mapping_rules_labels_t\n",
    "\n",
    "mapping_rules_labels_t = get_mapping_rules_labels_t(rule2label, len(labels2ids))\n",
    "print(\"T matrix:\", mapping_rules_labels_t[0:5,:])\n",
    "print(\"shape:\", mapping_rules_labels_t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We derive our T matrix with 148 rules for 14 classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to check if there are any rules which assign to the same class. Indeed, there is one case: rule \"defib\" builds a rule for two different classes. In our T matrix, \"defib\" occures in two rows, since knodle cannot handle one rule assigned to multiple classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "rule\n",
      "defib    2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(len(np.unique(rules['rule'])) == len(rules['rule']))\n",
    "rules_size = rules.groupby('rule').size()\n",
    "print(rules_size[np.where(rules_size > 1)[0]])\n",
    "#rule defib appears for two different classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Z Matrix\n",
    "The Z matrix contains the information wheter a rule matches to an observation or not. Its shape is #instances x #rules. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rule_matches_z.lib']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_rule_matches_z (data: np.ndarray, num_rules: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Function calculates the z matrix (samples x rules)\n",
    "    data: np.array (reports)\n",
    "    output: sparse z matrix\n",
    "    \"\"\"\n",
    "    rule_matches_z = np.zeros([len(data), num_rules])\n",
    "    for ind in range(len(data)):\n",
    "        for rule, rule_id in rule2rule_id.items():\n",
    "            if rule in (data[ind]):\n",
    "                rule_matches_z[ind, rule_id] = 1\n",
    "    return rule_matches_z\n",
    "\n",
    "rule_matches_z = get_rule_matches_z(input_list[:,4], (len(rule2rule_id)+1))\n",
    "\n",
    "dump(rule_matches_z, \"rule_matches_z.lib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image encoding \n",
    "For the image encoding we try two different approaches:\n",
    "1. extract features using a pretrained model (without finetuning)\n",
    "2. finetune a pretrained model and then extract features\n",
    "\n",
    "## Extracting features using a pretrained model (without finetuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the implementation we use pytorch and the pretrained resnet50 from torchvision. \n",
    "The following class loads the data and transforms it in the way it is required for resnet50. It is written in the form such that it is compatible with torch.utils.data.DataLoader. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mimicDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, path):\n",
    "        'Initialization'\n",
    "        self.path = path\n",
    "        #self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.path)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        image = Image.open(\"\".join([\"physionet.org/files/mimic-cxr-jpg/2.0.0/\",self.path[index,3].replace(\".dcm\", \".jpg\")])).convert('RGB')\n",
    "        X = self.transform(image)\n",
    "        label = self.path[index,6]\n",
    "        \n",
    "        return X, torch.tensor(label)\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the resnet50 model remove the last layer, so the output is then the second last layer with dimension 1x2048.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['all_X.lib']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.resnet50(pretrained=True)\n",
    "modules = list(model.children())[:-1]\n",
    "model=torch.nn.Sequential(*modules)\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "    \n",
    "model.eval()\n",
    "#apply modified resnet50 to data\n",
    "dataloaders = DataLoader(mimicDataset(input_list[:n,:]), batch_size=n,num_workers=0)\n",
    "    \n",
    "data, labels = next(iter(dataloaders))\n",
    "with torch.no_grad():\n",
    "    features_var = model(data)\n",
    "    features = features_var.data \n",
    "    all_X = features.reshape(n,2048).numpy()\n",
    "    \n",
    "#save feature matrix\n",
    "dump(all_X, \"all_X.lib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning a pretrained CNN and extracting the second last layer as features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second approach of image encoding, we use the concept of transfer learning. \n",
    "Therefore, we take a pretrained CNN and continue training the model with our data. This saves a lot of time and leads to satisfying results even on a small dataset. \n",
    "For the implementation we again use pytorch and the pretrained resnet50 from torchvision. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we split the data according to the split given in mimic-cxr-jpg. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_train = input_list[input_list[:,5] == 'train',:]\n",
    "input_validate = input_list[input_list[:,5] == 'validate',:]\n",
    "input_test = input_list[input_list[:,5] == 'test',:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the dataset is unbalanced, we use a weighted sampler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = np.zeros(14)\n",
    "for i in range(14): class_counts[i] = sum(input_train[:,6]==i)\n",
    "weight = 1/class_counts\n",
    "sample_weights = np.array([weight[t] for t in input_train[:,6]])\n",
    "sample_weights = torch.from_numpy(sample_weights)\n",
    "sample_weights = sample_weights.double()\n",
    "sampler = torch.utils.data.WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {'train' : mimicDataset(input_train),\n",
    "           'val': mimicDataset(input_validate),\n",
    "           'test':  mimicDataset(input_test)}\n",
    "\n",
    "dataloaders = {'train': DataLoader(dataset['train'] , batch_size=4, num_workers=0, sampler = sampler),\n",
    "               'val': DataLoader(dataset['val'] , batch_size=4, num_workers=0 )}\n",
    "\n",
    "dataset_sizes = {x: len(dataset[x]) for x in ['train', 'val']}\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function whcih trains the model is taken from https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                if phase == 'val':\n",
    "                    print('predictions',preds)\n",
    "\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    print('Best val Acc: {:4f}'.format(best_acc), )\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we initialize the model:\n",
    "We change the last layer to 14, since we only have 14 classes. We choose the Cross Entropy Loss as an optimization cirterion, the adam optimizer and an adaptive learning rate. \n",
    "The model is finetuned using only two epochs, since more epochs would overfit the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1\n",
      "----------\n",
      "train Loss: 1.8165 Acc: 0.4488\n",
      "predictions tensor([ 8,  8,  1, 13])\n",
      "predictions tensor([8, 8, 1, 1])\n",
      "predictions tensor([13,  8, 13,  1])\n",
      "predictions tensor([13, 12])\n",
      "val Loss: 2.7613 Acc: 0.0714\n",
      "\n",
      "Epoch 1/1\n",
      "----------\n",
      "train Loss: 1.1641 Acc: 0.6416\n",
      "predictions tensor([3, 0, 5, 9])\n",
      "predictions tensor([0, 0, 0, 0])\n",
      "predictions tensor([ 9,  0,  9, 13])\n",
      "predictions tensor([13, 13])\n",
      "val Loss: 2.8807 Acc: 0.0714\n",
      "\n",
      "Best val Acc: 0.071429\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet50(pretrained=True)\n",
    "num_ftrs = model.fc.in_features\n",
    "# set output size to 14 (number of classes)\n",
    "model.fc = nn.Linear(num_ftrs, 14)\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "step_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "model = train_model(model, criterion, optimizer, step_lr_scheduler, num_epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that we have trained our model, we will extract the features. \n",
    "\n",
    "In the next step, the last layer is removed from the model, so the output is then the second last layer with dimension 1x2048.\n",
    "Then we let all out data run through the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "modules = list(model.children())[:-1]\n",
    "model=torch.nn.Sequential(*modules)\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "    \n",
    "model.eval()\n",
    "#apply modified resnet50 to data\n",
    "dataloaders = DataLoader(mimicDataset(input_list[:n,:]), batch_size=n,num_workers=0)\n",
    "    \n",
    "data, labels = next(iter(dataloaders))\n",
    "with torch.no_grad():\n",
    "    features_var = model(data)\n",
    "    features = features_var.data \n",
    "    all_X_finetuned = features.reshape(n,2048).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['all_X_finetuned.lib']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#save feature matrix\n",
    "dump(all_X_finetuned, \"all_X_finetuned.lib\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
