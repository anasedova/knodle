{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "obvious-chicago",
   "metadata": {},
   "source": [
    "# Auto Trainer Tutorial\n",
    "\n",
    "Here we want to show you a really easy example how to use Knodle out-of-the box. This tutorial consists of three main parts:\n",
    "1. Download from Knodle Server and load into memory.\n",
    "2. Initialize Model (DistilBert) and prepare data for model input.\n",
    "3. Use AutoTrainer to run a simple training run\n",
    "\n",
    "All the steps are discussed in more detail below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "responsible-gossip",
   "metadata": {},
   "source": [
    "### Download data\n",
    " \n",
    "We will use a preprocessed version of the IMDb dataset. In https://github.com/knodle/knodle/examples, you can find a tutorial showing you how the data was preprossed and transformed into the Knodle format. Instead of using the download in the cells below, you can also use this tutorial to create the data yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frank-economy",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "\n",
    "imdb_data_dir = os.path.join(os.getcwd(), \"data\", \"imdb\")\n",
    "processed_data_dir = os.path.join(imdb_data_dir, \"processed\")\n",
    "os.makedirs(processed_data_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silver-venice",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minio import Minio\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "client = Minio(\"knodle.dm.univie.ac.at\", secure=False)\n",
    "files = [\n",
    "    \"df_train.csv\", \"df_dev.csv\", \"df_test.csv\",\n",
    "    \"train_rule_matches_z.lib\", \"dev_rule_matches_z.lib\", \"test_rule_matches_z.lib\",\n",
    "    \"mapping_rules_labels_t.lib\"\n",
    "]\n",
    "\n",
    "for file in tqdm(files):\n",
    "    client.fget_object(\n",
    "        bucket_name=\"knodle\",\n",
    "        object_name=os.path.join(\"datasets/imdb/processed\", file),\n",
    "        file_path=os.path.join(processed_data_dir, file),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "natural-slope",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_csv(os.path.join(processed_data_dir, \"df_train.csv\"))\n",
    "df_dev = pd.read_csv(os.path.join(processed_data_dir, \"df_dev.csv\"))\n",
    "df_test = pd.read_csv(os.path.join(processed_data_dir, \"df_test.csv\"))\n",
    "\n",
    "mapping_rules_labels_t = joblib.load(os.path.join(processed_data_dir, \"mapping_rules_labels_t.lib\"))\n",
    "\n",
    "train_rule_matches_z = joblib.load(os.path.join(processed_data_dir, \"train_rule_matches_z.lib\"))\n",
    "dev_rule_matches_z = joblib.load(os.path.join(processed_data_dir, \"dev_rule_matches_z.lib\"))\n",
    "test_rule_matches_z = joblib.load(os.path.join(processed_data_dir, \"test_rule_matches_z.lib\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operational-presence",
   "metadata": {},
   "source": [
    "### Data description\n",
    "\n",
    "We have three splits: train, develop and test split. For each split, there is DataFrame, holding text, and a Z matrix, relating instances, or rows in the DataFrame, to rules. Again, for more information we refer to the creation of the Dataset https://github.com/knodle/knodle/example.\n",
    "The training DataFrame only holds text, whereas the development and test set hold text and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unknown-roots",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ethical-design",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unusual-medicine",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train Z dimension: {train_rule_matches_z.shape}\")\n",
    "print(f\"Train avg. matches per sample: {train_rule_matches_z.sum() / train_rule_matches_z.shape[0]}\")\n",
    "print(f\"Develop avg. matches per sample: {dev_rule_matches_z.sum() / dev_rule_matches_z.shape[0]}\")\n",
    "print(f\"Test avg. matches per sample: {test_rule_matches_z.sum() / test_rule_matches_z.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "included-reservation",
   "metadata": {},
   "source": [
    "### Preprocess data to DistilBert input\n",
    "\n",
    "- Tokenize text. See https://huggingface.co/transformers/ on how to use Transformer-based models.\n",
    "- Transform data to PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "after-thousand",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "\n",
    "def convert_text_to_transformer_input(tokenizer, texts: List[str]) -> TensorDataset:\n",
    "    encoding = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    input_ids = encoding.get('input_ids')\n",
    "    attention_mask = encoding.get('attention_mask')\n",
    "\n",
    "    input_values_x = TensorDataset(input_ids, attention_mask)\n",
    "\n",
    "    return input_values_x\n",
    "\n",
    "\n",
    "def np_array_to_tensor_dataset(x: np.ndarray) -> TensorDataset:\n",
    "    if isinstance(x, sp.csr_matrix):\n",
    "        x = x.toarray()\n",
    "    x = torch.from_numpy(x)\n",
    "    x = TensorDataset(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broke-delay",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "X_train = convert_text_to_transformer_input(tokenizer, df_train[\"sample\"].tolist())\n",
    "X_dev = convert_text_to_transformer_input(tokenizer, df_dev[\"sample\"].tolist())\n",
    "X_test = convert_text_to_transformer_input(tokenizer, df_test[\"sample\"].tolist())\n",
    "\n",
    "y_dev = np_array_to_tensor_dataset(df_dev['label'].values)\n",
    "y_test = np_array_to_tensor_dataset(df_test['label'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tropical-renewal",
   "metadata": {},
   "source": [
    "## Training and evaluation\n",
    "\n",
    "The following code shows the usage of the majority trainer. It is a simple baseline, using the following steps:\n",
    "\n",
    "1. Restrict data to samples where at least one rule matches\n",
    "2. Use majority vote for cases where multiple rules match. If there's no clear winner, randomly choose between labels.\n",
    "3. Train DistilBert on these weakly formed labels. See https://huggingface.co/transformers/ on how to use Transformer-based models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interracial-extent",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AdamW\n",
    "\n",
    "#from knodle.trainer import AutoTrainer, AutoConfig\n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "trainer_type = \"majority\"\n",
    "custom_model_config = AutoConfig.create_config(\n",
    "    name=trainer_type,\n",
    "    optimizer=AdamW,\n",
    "    lr=1e-4,\n",
    "    batch_size=16,\n",
    "    epochs=2,\n",
    "    filter_non_labelled=True\n",
    ")\n",
    "\n",
    "print(custom_model_config)\n",
    "\n",
    "trainer = AutoTrainer(\n",
    "    name=\"majority\",\n",
    "    model=model,\n",
    "    mapping_rules_labels_t=mapping_rules_labels_t,\n",
    "    model_input_x=X_train,\n",
    "    rule_matches_z=train_rule_matches_z,\n",
    "    dev_model_input_x=X_dev,\n",
    "    dev_gold_labels_y=y_dev,\n",
    "    trainer_config=custom_model_config,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solid-church",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dict, _ = trainer.test(X_test, y_test)\n",
    "print(f\"Accuracy: {eval_dict.get('accuracy')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "treated-mining",
   "metadata": {},
   "source": [
    "## Further readings\n",
    "\n",
    "1. Go to the examples/ folder to see further tutorials.\n",
    "2. In order, the next tutorial would be Crossweigh/\n",
    "3. Finally, we want to encourage you to head over to our repository \n",
    "[knodle-experiments](https://github.com/knodle/knodle-experiments)\n",
    "which adds a new layer of abstraction on top of Knodle, allowing you to easily create full benchmarking setups."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
